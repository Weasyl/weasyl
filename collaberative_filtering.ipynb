{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weasyl Collaborative Filtering Example\n",
    "-----------------------------------------\n",
    "\n",
    "This notebook contains an example of user to user collaborative filtering on weasyl. This is something I did a few years ago successfully, but not in a performant manner: It would take tens of seconds to find recommendations for just one user.\n",
    "\n",
    "This is an attempt to revisit the problem with more performant python libraries.\n",
    "\n",
    "Before running this notebook, you'll want to create a .csv file with all the favorites from within postgresql as follows:\n",
    "```\n",
    "weasyl=# COPY (SELECT userid, targetid FROM favorite WHERE type='s') TO '/tmp/favorites.csv' DELIMITER ',' CSV HEADER;\n",
    "COPY 4389453\n",
    "```\n",
    "or unzip a favorites.csv.gz that I provide.\n",
    "\n",
    "You'll also need to install `numpy`, `pandas`, `scipy` and `ipython[notebook]` inside your ve to run this code.\n",
    "\n",
    "I consulted a few sources for this to see what other people have done. The most useful resource was http://blog.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/ which I've tried to modify here to use a sparse scipy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as npl\n",
    "import pandas as pd\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time I did this, we used unary ratings (i.e. every submission was either favorited by a user or not). As such it made sense to use the [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index) and treat everything in terms of sets.\n",
    "\n",
    "However, going forward we want to support favorites, likes, and dislikes. So treat all favorites as score `2` (likes will eventually be 1 and dislikes as -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>targetid</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  targetid  rating\n",
       "0       3        23       2\n",
       "1       3        24       2\n",
       "2       3        25       2\n",
       "3       3        29       2\n",
       "4      10        25       2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('favorites.csv')\n",
    "df['rating'] = 2  # Everything is favorites currently\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created a dataframe, we want to create a sparse matrix from it. Since our userids and targetids are not contiguous (e.g. the lowest userid who has favorited anything is 3 and not every user has favorites and many favorites are anonymized when we export the db, etc.), we'll make a mapping of new indices to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0,\n",
       " 6: 3784,\n",
       " 10: 1,\n",
       " 12: 43,\n",
       " 13: 1242,\n",
       " 15: 3,\n",
       " 17: 2,\n",
       " 20: 158,\n",
       " 21: 266,\n",
       " 131083: 37171}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_map = {x[1]: x[0] for x in enumerate(df.userid.unique())}\n",
    "item_map = {x[1]: x[0] for x in enumerate(df.targetid.unique())}\n",
    "\n",
    "# Show a few items\n",
    "{k: user_map[k] for k in user_map.keys()[:10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct our sparse matrix. This will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38903 users.\n",
      "857382 items.\n"
     ]
    }
   ],
   "source": [
    "n_users = df.userid.unique().shape[0]\n",
    "n_items = df.targetid.unique().shape[0]\n",
    "assert n_users == len(user_map)\n",
    "assert n_items == len(item_map)\n",
    "\n",
    "print(\"{} users.\".format(n_users))\n",
    "print(\"{} items.\".format(n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = scipy.sparse.csr_matrix((df['rating'], (df.userid.map(user_map), df.targetid.map(item_map))),\n",
    "                                  shape=(n_users, n_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below was adapted from the blog post linked above. See: http://blog.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/#Collaborative-filtering\n",
    "\n",
    "I've made two changes:\n",
    " * I no longer use epsilon because scipy dies when I try to add a scalar to a sparse matrix. This shouldn't be matter because all users in the matrix have at least one favorite.\n",
    " * I use `sim.diagonal()` instead of `np.diag()` because sparse matrices complain mightily when it comes to `np.diag()`\n",
    " * Things die if we try to do regular division. Instead, we construct a diagonal matrix with the\n",
    "   reciprocals of everything we would have divided with and both pre-multiply (to scale every row by the first user) and post multiply (to scale every column by the second user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast_similarity(ratings, kind='user', epsilon=1e-9):\n",
    "    # epsilon -> small number for handling dived-by-zero errors\n",
    "    if kind == 'user':\n",
    "        sim = ratings.dot(ratings.T)\n",
    "    elif kind == 'item':\n",
    "        sim = ratings.T.dot(ratings)\n",
    "    norms = np.array([np.sqrt(sim.diagonal())])\n",
    "    norms_sparse_diag = scipy.sparse.diags(1/norms.ravel(), format='csr')\n",
    "    return (norms_sparse_diag * sim * norms_sparse_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated user similarities in 3.51231694221 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now calculate user-user similarities. This used to take quite a while but with the fixes above it's pretty fast.\n",
    "before = time.time()\n",
    "user_similarity = fast_similarity(ratings, kind='user')\n",
    "print(\"Calculated user similarities in {} seconds\".format(time.time() - before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have user-user similarities (e.g. the cosine similarity between all pairs of users). Use them to generate recommendations.\n",
    "\n",
    "Here we will again use the code from the blog post above. However, we must adapt it to sparse matrices.\n",
    "\n",
    "I've found that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_user_similarity = scipy.sparse.coo_matrix(user_similarity)\n",
    "# c_ratings = scipy.sparse.coo_matrix(ratings)\n",
    "# print(\"Starting large operation...\")\n",
    "# big = c_user_similarity.dot(c_ratings)\n",
    "SKYLER = user_map[2402]  # userid for `skylerbunny`.\n",
    "k = 20\n",
    "# Could make this more efficient with argpartition instead.\n",
    "top_friends = np.argsort(user_similarity[:,SKYLER].toarray().ravel())[-2:-k-2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.weasyl.com/submission/1123781\n",
      "https://www.weasyl.com/submission/798518\n",
      "https://www.weasyl.com/submission/958234\n",
      "https://www.weasyl.com/submission/1354967\n",
      "https://www.weasyl.com/submission/1251826\n",
      "https://www.weasyl.com/submission/1076761\n",
      "https://www.weasyl.com/submission/762669\n",
      "https://www.weasyl.com/submission/695179\n",
      "https://www.weasyl.com/submission/734759\n",
      "https://www.weasyl.com/submission/777435\n"
     ]
    }
   ],
   "source": [
    "# skyler_preds = np.zeros(ratings.shape[1])\n",
    "# print skyler_preds\n",
    "# for friend in top_friends:\n",
    "#     skyler_preds += ratings[friend, :].toarray() * user_similarity[SKYLER, friend]\n",
    "# # top_recs = np.argpartition(skyler_preds.toarray(), -100)[-100:]\n",
    "# print top_recs.shape\n",
    "# skyler_preds\n",
    "# print user_similarity[SKYLER, :][top_friends].shape\n",
    "# inp = user_similarity[SKYLER, :].toarray().ravel()\n",
    "rev_item_map = {v: k for k, v in item_map.iteritems()}\n",
    "preds = user_similarity[SKYLER, top_friends].dot(ratings[top_friends, :])\n",
    "\n",
    "for x in [rev_item_map[x] for x in np.argsort(preds.toarray().ravel()) if not ratings[SKYLER, x]][-10:]:\n",
    "    print(\"https://www.weasyl.com/submission/{}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the code as provided in the blog post above. It's going to run into similar speed/memory issues.\n",
    "def predict_fast_simple(ratings, similarity, kind='user'):\n",
    "    \n",
    "    if kind == 'user':\n",
    "        norms = scipy.sparse.diags((1/np.array([np.abs(similarity).sum(axis=1)]).T).ravel(), format='csr')\n",
    "        return norms * similarity.dot(ratings)\n",
    "    elif kind == 'item':\n",
    "        return ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-0d0413eec3ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0muser_similarity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#predictions = predict_fast_simple(ratings, user_similarity, kind='user')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calculated predictions in {} seconds.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbefore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hyena/repos/weasyl/weasyl-env/local/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36mdot\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \"\"\"\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hyena/repos/weasyl/weasyl-env/local/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;31m# If it's a list or whatever, treat it like a matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hyena/repos/weasyl/weasyl-env/local/lib/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36m_mul_sparse_matrix\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    538\u001b[0m                                     maxval=nnz)\n\u001b[1;32m    539\u001b[0m         \u001b[0mindptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nnz: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# With our user similarities in hand, let's calculate the predictions.\n",
    "# You should expect this to run for a long time as well.\n",
    "\n",
    "# TODO(hyena): There's no way this will work because it does calculate affinity for EVERYTHING. That matrix is too big.\n",
    "# The solution is probably to only use the top k-closest friends.\n",
    "# Until then this is disabled.\n",
    "\n",
    "before = time.time()\n",
    "user_similarity.dot(ratings)\n",
    "#predictions = predict_fast_simple(ratings, user_similarity, kind='user')\n",
    "print(\"Calculated predictions in {} seconds.\".format(time.time() - before))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38903\n",
      "38903\n",
      "38903\n",
      "0.0547175655165\n",
      "0.0547175655165\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate predictions for ONE user.\n",
    "# This would be faster if we didn't use a for-loop.\n",
    "SKYLER = user_map[2402]  # userid for `skylerbunny`.\n",
    "before = time.time()\n",
    "skyler_preds = np.zeros(ratings.shape[1])\n",
    "\n",
    "# This operation will be faster if we just use the k-closest friends.\n",
    "# Calculate them:\n",
    "k = 20\n",
    "\n",
    "# This is my attempt to get just the k highest friends. As you can see in the output, it's a disaster.\n",
    "top_k_friends = [np.argsort(user_similarity[SKYLER, :])[:-k-1:-1]]\n",
    "print top_k_friends[0].shape[1]\n",
    "print user_similarity.shape[0]\n",
    "print user_similarity.shape[1]\n",
    "print user_similarity[SKYLER, 30948]\n",
    "print user_similarity[30948, SKYLER]\n",
    "#for i in xrange(ratings.shape[1]):\n",
    "#    skyler_preds[i] = (user_similarity[SKYLER, :].dot(ratings[:, i])\n",
    "#                    / np.sum(np.abs(user_similarity[SKYLER, :])))\n",
    "#print(\"Calculated recommendations for Skylerbunny in {} seconds\"\n",
    "#      .format(time.time() - before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What we'd like to do at this point is some calculation like the following pseudo code:\n",
    "To calculate predictions for a given user, `i`:\n",
    "\n",
    "Look at the top `k` most similar users to `i`. Say that user `j` is amongst them and has similarity `sim_{ij}`. Then do a weighted average of all ratings for all `k` users where `j`'s ratings are weighted by `sim_{ij}`.\n",
    "\n",
    "Here's pseudocode showing a naive way this could be done with for loops and python dicts. Assume that we have:\n",
    " * `k_closest` is an array mapping userid keys to float similarity values\n",
    " * `score` is a list of dicts. `score[k][j]` is the score user `k` assigned to item `j`. There are no entries for unrated items.\n",
    "\n",
    "```\n",
    "scores = {}  # This will contain our predictions.\n",
    "\n",
    "for neighbor in k_closest:\n",
    "    for item, rating in score[neighbor].iteritems():\n",
    "        scores[item] = scores.get(item, 0) + rating * k_closest[neighbor]\n",
    "return sorted(scores, key=lambda item: -scores[item])  # Put the item with the highest score first.\n",
    "```\n",
    "\n",
    "You can see some of this idea [here](http://blog.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/#Top-$k$-Collaborative-Filtering)\n",
    "\n",
    "However, when I tried to do this, argsort wasn't behaving at all right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
