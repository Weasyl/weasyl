{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weasyl Collaborative Filtering Example\n",
    "-----------------------------------------\n",
    "\n",
    "This notebook contains an example of user to user collaborative filtering on weasyl. This is something I did a few years ago successfully, but not in a performant manner: It would take tens of seconds to find recommendations for just one user.\n",
    "\n",
    "This is an attempt to revisit the problem with more performant python libraries.\n",
    "\n",
    "Before running this notebook, you'll want to create a .csv file with all the favorites from within postgresql as follows:\n",
    "```\n",
    "weasyl=# COPY (SELECT userid, targetid FROM favorite WHERE type='s') TO '/tmp/favorites.csv' DELIMITER ',' CSV HEADER;\n",
    "COPY 4389453\n",
    "```\n",
    "or unzip a favorites.csv.gz that I provide.\n",
    "\n",
    "You'll also need to install `numpy`, `pandas`, `scipy` and `ipython[notebook]` inside your ve to run this code.\n",
    "\n",
    "I consulted a few sources for this to see what other people have done. The most useful resource was http://blog.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/ which I've tried to modify here to use a sparse scipy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time I did this, we used unary ratings (i.e. every submission was either favorited by a user or not). As such it made sense to use the [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index) and treat everything in terms of sets.\n",
    "\n",
    "However, going forward we want to support favorites, likes, and dislikes. So treat all favorites as score `2` (likes will eventually be 1 and dislikes will be -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>targetid</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  targetid  rating\n",
       "0       3        23       2\n",
       "1       3        24       2\n",
       "2       3        25       2\n",
       "3       3        29       2\n",
       "4      10        25       2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('favorites.csv')\n",
    "df['rating'] = 2  # Everything is favorites currently\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created a dataframe, we want to create a sparse matrix from it. Since our userids and targetids are not contiguous (e.g. the lowest userid who has favorited anything is 3 and not every user has favorites and many favorites are anonymized when we export the db, etc.), we'll make a mapping of new indices to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0,\n",
       " 6: 3784,\n",
       " 10: 1,\n",
       " 12: 43,\n",
       " 13: 1242,\n",
       " 15: 3,\n",
       " 17: 2,\n",
       " 20: 158,\n",
       " 21: 266,\n",
       " 131083: 37171}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_map = {x[1]: x[0] for x in enumerate(df.userid.unique())}\n",
    "item_map = {x[1]: x[0] for x in enumerate(df.targetid.unique())}\n",
    "\n",
    "# Show a few items\n",
    "{k: user_map[k] for k in user_map.keys()[:10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct our sparse matrix. This will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38903 users.\n",
      "857382 items.\n"
     ]
    }
   ],
   "source": [
    "n_users = df.userid.unique().shape[0]\n",
    "n_items = df.targetid.unique().shape[0]\n",
    "assert n_users == len(user_map)\n",
    "assert n_items == len(item_map)\n",
    "\n",
    "print(\"{} users.\".format(n_users))\n",
    "print(\"{} items.\".format(n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = scipy.sparse.csr_matrix((df['rating'], (df.userid.map(user_map), df.targetid.map(item_map))),\n",
    "                                  shape=(n_users, n_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below was adapted from the blog post linked above. See: http://blog.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/#Collaborative-filtering\n",
    "\n",
    "I've made two changes:\n",
    " * I no longer use epsilon because scipy dies when I try to add a scalar to a sparse matrix. This shouldn't be matter because all users in the matrix have at least one favorite.\n",
    " * I use `sim.diagonal()` instead of `np.diag()` because sparse matrices complain mightily when it comes to `np.diag()`\n",
    " * Things die if we try to do regular division. Instead, we construct a diagonal matrix with the\n",
    "   reciprocals of everything we would have divided with and both pre-multiply (to scale every row by the first user) and post multiply (to scale every column by the second user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast_similarity(ratings, kind='user', epsilon=1e-9):\n",
    "    # epsilon -> small number for handling dived-by-zero errors\n",
    "    if kind == 'user':\n",
    "        sim = ratings.dot(ratings.T)\n",
    "    elif kind == 'item':\n",
    "        sim = ratings.T.dot(ratings)\n",
    "    norms = np.array([np.sqrt(sim.diagonal())])\n",
    "    norms_sparse_diag = scipy.sparse.diags(1/norms.ravel(), format='csr')\n",
    "    return (norms_sparse_diag * sim * norms_sparse_diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User similarities calculated in 2.99600410461 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Now calculate user-user similarities. This used to take quite a while but with the fixes above it's pretty fast.\n",
    "before = time.time()\n",
    "user_similarity = fast_similarity(ratings, kind='user')\n",
    "print(\"User similarities calculated in {} seconds.\".format(time.time() - before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have user-user similarities (e.g. the cosine similarity between all pairs of users). Use them to generate recommendations.\n",
    "\n",
    "Here we will again use the code from the blog post above. However, we must adapt it to sparse matrices.\n",
    "\n",
    "I've found that either due to a memory leak in my version of scipy (0.19 on Ubuntu 64-bit) or due to the number of non-zero elements in the full recommendation array, we can't calculate all the recommendations at once.\n",
    "\n",
    "So for now we'll write a method that calculates it for one user at a time as needed. This method will only use the k-most similar users to make its suggestions.\n",
    "\n",
    "In practice this usually creates better results, but we still have to run cross validation to confirm that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rev_item_map = {v: k for k, v in item_map.iteritems()}\n",
    "\n",
    "def recs_for_user(userid, similarity, ratings, k=20, count=10):\n",
    "    \"\"\"\n",
    "    Generates recommendations for a weasyl user.\n",
    "    \n",
    "    Args:\n",
    "        userid (int): The weasyl userid.\n",
    "        similarity (matrix): The user to user similarity matrix.\n",
    "        ratings (matrix): The user x item rating matrix.\n",
    "        k (int, optional): How many closest other users to use. Defaults to 20.\n",
    "        count (int, optional): How many items to return for the user. Defaults to 10.\n",
    "    \n",
    "    Returns:\n",
    "        An array of weasyl submission ids for the user.\n",
    "    \"\"\"\n",
    "    if userid not in user_map:\n",
    "        print(\"User has favorited anything.\")\n",
    "        return []\n",
    "    user_index = user_map[userid]\n",
    "    top_friends = np.argpartition(similarity[:,user_index].toarray().ravel(), -k)[-2:-k-2:-1]\n",
    "    top_friends = top_friends[top_friends != user_index]  # Don't use ourselves for recommendations.\n",
    "    # TODO: Don't include columns for items we've rated ourselves.\n",
    "    preds = similarity[user_index, top_friends].dot(ratings[top_friends, :])\n",
    "    # TODO: Use argpartition to speed this up\n",
    "    return [rev_item_map[x] for x in np.argsort(preds.toarray().ravel())[-count:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this function to generate recommendations. Filter out things we've already favorited ourselves.\n",
    "In the future we should do that before calculating recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations generated in 0.0863590240479 seconds\n",
      "https://www.weasyl.com/submission/1114995\n",
      "https://www.weasyl.com/submission/1364444\n",
      "https://www.weasyl.com/submission/880203\n",
      "https://www.weasyl.com/submission/883949\n",
      "https://www.weasyl.com/submission/689866\n",
      "https://www.weasyl.com/submission/1239404\n",
      "https://www.weasyl.com/submission/882610\n"
     ]
    }
   ],
   "source": [
    "before = time.time()\n",
    "recs = recs_for_user(2061, user_similarity, ratings, count=20)\n",
    "print(\"Recommendations generated in {} seconds\".format(time.time() - before))\n",
    "for x in recs:\n",
    "    if ratings[user_map[2061], item_map[x]]:\n",
    "        continue  # Don't include our own favorites.\n",
    "    print(\"https://www.weasyl.com/submission/{}\".format(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
