{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weasyl Collaborative Filtering Example\n",
    "-----------------------------------------\n",
    "\n",
    "This notebook contains an example of user to user collaborative filtering on weasyl. This is something I did a few years ago successfully, but not in a performant manner: It would take tens of seconds to find recommendations for just one user.\n",
    "\n",
    "This is an attempt to revisit the problem with more performant python libraries.\n",
    "\n",
    "Before running this notebook, you'll want to create a .csv file with all the favorites from within postgresql as follows:\n",
    "```\n",
    "weasyl=# COPY (SELECT userid, targetid FROM favorite WHERE type='s') TO '/tmp/favorites.csv' DELIMITER ',' CSV HEADER;\n",
    "COPY 4389453\n",
    "```\n",
    "or unzip a favorites.csv.gz that I provide.\n",
    "\n",
    "You'll also need to install `numpy`, `pandas`, `scipy` and `ipython[notebook]` inside your ve to run this code.\n",
    "\n",
    "I consulted a few sources for this to see what other people have done. The most useful resource was http://blog.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/ which I've tried to modify here to use a sparse scipy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time I did this, we used unary ratings (i.e. every submission was either favorited by a user or not). As such it made sense to use the [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index) and treat everything in terms of sets.\n",
    "\n",
    "However, going forward we want to support favorites, likes, and dislikes. So treat all favorites as score `2` (likes will eventually be 1 and dislikes as -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>targetid</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  targetid  rating\n",
       "0       3        23       2\n",
       "1       3        24       2\n",
       "2       3        25       2\n",
       "3       3        29       2\n",
       "4      10        25       2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('favorites.csv')\n",
    "df['rating'] = 2  # Everything is favorites currently\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created a dataframe, we want to create a sparse matrix from it. Since our userids and targetids are not contiguous (e.g. the lowest userid who has favorited anything is 3 and not every user has favorites and many favorites are anonymized when we export the db, etc.), we'll make a mapping of new indices to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 0,\n",
       " 6: 3784,\n",
       " 10: 1,\n",
       " 12: 43,\n",
       " 13: 1242,\n",
       " 15: 3,\n",
       " 17: 2,\n",
       " 20: 158,\n",
       " 21: 266,\n",
       " 131083: 37171}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_map = {x[1]: x[0] for x in enumerate(df.userid.unique())}\n",
    "item_map = {x[1]: x[0] for x in enumerate(df.targetid.unique())}\n",
    "\n",
    "# Show a few items\n",
    "{k: user_map[k] for k in user_map.keys()[:10]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now construct our sparse matrix. This will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38903 users.\n",
      "857382 items.\n"
     ]
    }
   ],
   "source": [
    "n_users = df.userid.unique().shape[0]\n",
    "n_items = df.targetid.unique().shape[0]\n",
    "assert n_users == len(user_map)\n",
    "assert n_items == len(item_map)\n",
    "\n",
    "print(\"{} users.\".format(n_users))\n",
    "print(\"{} items.\".format(n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = scipy.sparse.csr_matrix((df['rating'], (df.userid.map(user_map), df.targetid.map(item_map))),\n",
    "                                  shape=(n_users, n_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below was adapted from the blog post linked above. See: http://blog.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/#Collaborative-filtering\n",
    "\n",
    "I've made two changes:\n",
    " * I no longer use epsilon because scipy dies when I try to add a scalar to a sparse matrix\n",
    " * I use `sim.diagonal()` instead of `np.diag()` because sparse matrices complain mightily when it comes to `np.diag()`\n",
    " \n",
    "Not sure if this is fully correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast_similarity(ratings, kind='user', epsilon=1e-9):\n",
    "    # epsilon -> small number for handling dived-by-zero errors\n",
    "    if kind == 'user':\n",
    "        sim = ratings.dot(ratings.T)\n",
    "    elif kind == 'item':\n",
    "        sim = ratings.T.dot(ratings)\n",
    "    norms = np.array([np.sqrt(sim.diagonal())])\n",
    "    return (sim / norms / norms.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated user similarities in 339.22723794 seconds\n"
     ]
    }
   ],
   "source": [
    "# Now calculate user-user similarities. This will take a while during which time your computer may be unresponsive.\n",
    "before = time.time()\n",
    "user_similarity = fast_similarity(ratings, kind='user')\n",
    "print(\"Calculated user similarities in {} seconds\".format(time.time() - before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have user-user similarities (e.g. the cosine similarity between all pairs of users). Use them to generate recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_fast_simple(ratings, similarity, kind='user'):\n",
    "    if kind == 'user':\n",
    "        return similarity.dot(ratings) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif kind == 'item':\n",
    "        return ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated predictions in 7.48634338379e-05 seconds.\n"
     ]
    }
   ],
   "source": [
    "# With our user similarities in hand, let's calculate the predictions.\n",
    "# You should expect this to run for a long time as well.\n",
    "\n",
    "# TODO(hyena): There's no way this will work because it does calculate affinity for EVERYTHING. That matrix is too big.\n",
    "# The solution is probably to only use the top k-closest friends.\n",
    "# Until then this is disabled.\n",
    "\n",
    "before = time.time()\n",
    "#predictions = predict_fast_simple(ratings, user_similarity, kind='user')\n",
    "print(\"Calculated predictions in {} seconds.\".format(time.time() - before))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_map' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-34ccdb7cadff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Let's calculate predictions for ONE user.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This would be faster if we didn't use a for-loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mSKYLER\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2402\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# userid for `skylerbunny`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mskyler_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'user_map' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's calculate predictions for ONE user.\n",
    "# This would be faster if we didn't use a for-loop.\n",
    "SKYLER = user_map[2402]  # userid for `skylerbunny`.\n",
    "before = time.time()\n",
    "skyler_preds = np.zeros(ratings.shape[1])\n",
    "\n",
    "# This operation will be faster if we just use the k-closest friends.\n",
    "# Calculate them:\n",
    "k = 20\n",
    "\n",
    "# This is my attempt to get just the k highest friends. As you can see in the output, it's a disaster.\n",
    "top_k_friends = [np.argsort(user_similarity[SKYLER, :])[:-k-1:-1]]\n",
    "print top_k_friends[0].shape[1]\n",
    "print user_similarity.shape[0]\n",
    "print user_similarity.shape[1]\n",
    "print user_similarity[SKYLER, 30948]\n",
    "print user_similarity[30948, SKYLER]\n",
    "#for i in xrange(ratings.shape[1]):\n",
    "#    skyler_preds[i] = (user_similarity[SKYLER, :].dot(ratings[:, i])\n",
    "#                    / np.sum(np.abs(user_similarity[SKYLER, :])))\n",
    "#print(\"Calculated recommendations for Skylerbunny in {} seconds\"\n",
    "#      .format(time.time() - before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What we'd like to do at this point is some calculation like the following pseudo code:\n",
    "To calculate predictions for a given user, `i`:\n",
    "\n",
    "Look at the top `k` most similar users to `i`. Say that user `j` is amongst them and has similarity `sim_{ij}`. Then do a weighted average of all ratings for all `k` users where `j`'s ratings are weighted by `sim_{ij}`.\n",
    "\n",
    "Here's pseudocode showing a naive way this could be done with for loops and python dicts. Assume that we have:\n",
    " * `k_closest` is an array mapping userid keys to float similarity values\n",
    " * `score` is a list of dicts. `score[k][j]` is the score user `k` assigned to item `j`. There are no entries for unrated items.\n",
    "\n",
    "```\n",
    "scores = {}  # This will contain our predictions.\n",
    "\n",
    "for neighbor in k_closest:\n",
    "    for item, rating in score[neighbor].iteritems():\n",
    "        scores[item] = scores.get(item, 0) + rating * k_closest[neighbor]\n",
    "return sorted(scores, key=lambda item: -scores[item])  # Put the item with the highest score first.\n",
    "```\n",
    "\n",
    "You can see some of this idea [here](http://blog.ethanrosenthal.com/2015/11/02/intro-to-collaborative-filtering/#Top-$k$-Collaborative-Filtering)\n",
    "\n",
    "However, when I tried to do this, argsort wasn't behaving at all right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
